inherit:
  - datasets/rule-gpt2.yaml
  - models/mistral-small.yaml
  - trainers/gpt2-small.yaml

run_id: full-rule-run

artifacts:
  cache_dir: ./cache
  run_dir: ./runs/full-rule-run

effective_bsz: 8

resume: true
resume_checkpoint: null

checkpoint_frequency:
  - [100, 1000]
  - [500, 5000]

seed: 42

training_arguments:
  max_steps: 2000
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  logging_steps: 10
  eval_steps: 200
  save_steps: 200
  fp16: false
  learning_rate: 5e-4
  weight_decay: 0.01
  warmup_steps: 100
  lr_scheduler_type: linear
  prediction_loss_only: true
  dataloader_num_workers: 0
