#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Train a causal LM from a YAML (generated by tools/generate_yaml.py).
Usage:
  python -m training.train_lm \
    --input_dir data/perturbed_local/local_Anone-forward_Panimal-forward
"""

import os
import csv
import json
import math
import time
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional

import yaml
import torch
from datasets import load_dataset, concatenate_datasets
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
    TrainerCallback,
)
from utils import CHECKPOINT_PATH, CONFIG_PATH, CACHE_PATH, AGENT_MARK, PATIENT_MARK

# ---- 防碎片化设置 ----
if "PYTORCH_CUDA_ALLOC_CONF" not in os.environ:
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64"
print(f"[Info] Set PYTORCH_CUDA_ALLOC_CONF={os.environ['PYTORCH_CUDA_ALLOC_CONF']}")

torch.cuda.empty_cache()

# ---- perf knobs ----
torch.set_num_threads(1)
torch.set_num_interop_threads(1)
if torch.backends.cudnn.is_available():
    torch.backends.cudnn.benchmark = True
try:
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.set_float32_matmul_precision("high")
except Exception:
    pass


# -------------------------
# Helpers
# -------------------------

def _expand(p: str) -> str:
    return os.path.expandvars(os.path.expanduser(p))

def _build_dataset(files: List[str]):
    if not files:
        return None
    ds_list = []
    for f in files:
        f = _expand(f)
        if f.endswith(".txt"):
            ds = load_dataset("text", data_files=f, split="train")
        elif f.endswith(".jsonl") or f.endswith(".json"):
            ds = load_dataset("json", data_files=f, split="train")
            if "text" not in ds.column_names:
                raise ValueError(f"{f} 需要包含字段 'text'")
        else:
            raise ValueError(f"不支持的文件类型：{f}")
        ds_list.append(ds)
    return concatenate_datasets(ds_list) if len(ds_list) > 1 else ds_list[0]

def _group_texts(examples: Dict[str, list], block_size: int):
    keys = [k for k, v in examples.items() if isinstance(v, list) and v and isinstance(v[0], list)]
    concatenated = {k: sum(examples[k], []) for k in keys}
    total_len = len(concatenated.get("input_ids", []))
    total_len = (total_len // block_size) * block_size
    result = {k: [t[i:i + block_size] for i in range(0, total_len, block_size)] for k, t in concatenated.items()}
    if "input_ids" not in result:
        result["input_ids"] = []
    result["labels"] = result["input_ids"].copy()
    return result

def _find_last_checkpoint(run_dir: Path) -> Optional[Path]:
    if not run_dir.exists():
        return None
    cks = [p for p in run_dir.glob("checkpoint-*") if p.is_dir()]
    if not cks:
        return None
    cks.sort(key=lambda p: int(p.name.split("-")[-1]))
    return cks[-1]

# ✅ 修改版：流式统计 tokens，避免内存爆炸
def _count_total_tokens(tokenized_ds, batch_size: int = 1000) -> int:
    total = 0
    for batch in tokenized_ds.iter(batch_size=batch_size):
        total += sum(len(x) for x in batch["input_ids"])
    return total


# -------------------------
# Callbacks
# -------------------------

class ValidPPLLogger(TrainerCallback):
    def __init__(self, csv_path: Path):
        self.csv_path = csv_path
        self._csv_path_str = str(csv_path)
        csv_path.parent.mkdir(parents=True, exist_ok=True)
        if not csv_path.exists():
            with open(self._csv_path_str, "w", newline="") as f:
                csv.writer(f).writerow(["step", "eval_loss", "ppl", "wall_time"])
        self.tb_writer = None

    def on_evaluate(self, args, state, control, metrics, **kwargs):
        if "eval_loss" not in metrics:
            return
        eval_loss = float(metrics["eval_loss"])
        ppl = float(math.exp(eval_loss))
        step = int(state.global_step)
        with open(self._csv_path_str, "a", newline="") as f:
            csv.writer(f).writerow([step, eval_loss, ppl, time.time()])
        if state.is_world_process_zero:
            print(f"[Eval] step={step} eval_loss={eval_loss:.6f} ppl={ppl:.3f}")
        try:
            if self.tb_writer is not None:
                self.tb_writer.add_scalar("eval_ppl", ppl, step)
        except Exception:
            pass


class ThroughputLogger(TrainerCallback):
    def __init__(self, tokens_per_step: int):
        self.tokens_per_step = tokens_per_step
        self.t0 = None

    def on_step_begin(self, args, state, control, **kwargs):
        self.t0 = time.time()

    def on_step_end(self, args, state, control, **kwargs):
        if self.t0 is None:
            return
        dt = time.time() - self.t0
        tps = self.tokens_per_step / max(dt, 1e-6)
        if state.is_world_process_zero and (state.global_step % max(1, args.logging_steps) == 0):
            print(f"[Speed] step={state.global_step} ~{tps:,.0f} tokens/s")


class DynamicCheckpointSaver(TrainerCallback):
    def __init__(self, freq_list, output_dir: Path):
        self.freq_list = freq_list
        self.output_dir = output_dir

    def _get_interval(self, step: int) -> Optional[int]:
        for interval, max_step in self.freq_list:
            if step <= max_step:
                return interval
        return None

    def on_step_end(self, args, state, control, **kwargs):
        interval = self._get_interval(state.global_step)
        if interval is not None and state.global_step % interval == 0:
            control.should_save = True
        return control


# -------------------------
# Main
# -------------------------

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--input_dir", type=str, required=True,
                    help="Perturbation output directory (e.g., data/perturbed_local/local_Anone-forward_Panimal-forward)")
    args = ap.parse_args()

    base_dir = Path(args.input_dir).resolve()
    if not base_dir.exists():
        raise FileNotFoundError(f"Directory not found: {base_dir}")

    run_id = base_dir.name
    cfg_path = Path(CONFIG_PATH) / f"{run_id}.yaml"
    if not cfg_path.exists():
        raise FileNotFoundError(f"Config not found: {cfg_path}. Run tools/generate_yaml first.")

    with open(cfg_path, "r", encoding="utf-8") as f:
        cfg: Dict[str, Any] = yaml.safe_load(f)

    data_cfg: Dict[str, Any] = cfg.get("data", {})
    train_files: List[str] = data_cfg.get("train_files", [])
    eval_files: List[str] = data_cfg.get("eval_files", [])

    run_dir = Path(cfg.get("artifacts", {}).get("run_dir", str(Path(CHECKPOINT_PATH) / run_id)))
    run_dir = Path(_expand(str(run_dir)))
    run_dir.mkdir(parents=True, exist_ok=True)

    cache_dir = Path(CACHE_PATH) / run_id
    cache_dir.mkdir(parents=True, exist_ok=True)
    os.environ["HF_HOME"] = str(cache_dir)
    os.environ["HF_DATASETS_CACHE"] = str(cache_dir / "datasets")

    targs = dict(cfg.get("training_arguments", {}))
    if "save_total_limit" in targs:
        targs.pop("save_total_limit")

    model_name = cfg.get("model_name", "gpt2")
    block_size = int(cfg.get("block_size", 1024))
    seed = int(cfg.get("seed", 42))
    effective_bsz = cfg.get("effective_bsz", None)

    print("[Info] ===== Training Config =====")
    print(f"[Info] run_id={run_id}")
    print(f"[Info] model={model_name}")
    print(f"[Info] block_size={block_size}, seed={seed}")
    print(f"[Info] train_files={len(train_files)} | eval_files={len(eval_files)}")
    print(f"[Info] output_dir={run_dir}")
    print(f"[Info] cache_dir={cache_dir}")
    print("=================================")

    if effective_bsz is not None:
        pbsz = int(targs.get("per_device_train_batch_size", 1))
        gas = math.ceil(float(effective_bsz) / max(pbsz, 1))
        targs["gradient_accumulation_steps"] = int(gas)

    if eval_files:
        targs["evaluation_strategy"] = "steps"
        targs.setdefault("save_strategy", "steps")

    train_ds = _build_dataset(train_files)
    if train_ds is None:
        raise RuntimeError("train_files 为空")
    eval_ds = _build_dataset(eval_files) if eval_files else None

    # -------- tokenizer / model --------
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, cache_dir=str(cache_dir))
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # 加 special tokens
    to_add = []
    for mark in [AGENT_MARK, PATIENT_MARK]:
        if mark not in tokenizer.get_vocab():
            to_add.append(mark)
    if to_add:
        num_added = tokenizer.add_special_tokens({"additional_special_tokens": to_add})
        print(f"[Info] Added {num_added} special tokens to tokenizer:", to_add)
    else:
        print("[Info] Special tokens already in vocab.")

    model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=str(cache_dir))
    if len(tokenizer) != model.get_input_embeddings().weight.size(0):
        model.resize_token_embeddings(len(tokenizer))
        print(f"[Info] Resized model embeddings to {len(tokenizer)}")

    for m in [AGENT_MARK, PATIENT_MARK]:
        toks = tokenizer.tokenize(m)
        print(f"[Check] token '{m}' -> {toks} | in_vocab={m in tokenizer.get_vocab()}")

    if bool(targs.get("gradient_checkpointing", False)):
        try:
            model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={"use_reentrant": False})
        except TypeError:
            model.gradient_checkpointing_enable()
        model.config.use_cache = False

    def tok_fn(batch):
        return tokenizer(batch["text"], return_attention_mask=False)

    train_tok = train_ds.map(tok_fn, batched=True, remove_columns=train_ds.column_names, num_proc=4)
    eval_tok = eval_ds.map(tok_fn, batched=True, remove_columns=eval_ds.column_names, num_proc=4) if eval_ds else None

    # ✅ 流式 token 统计
    raw_train_tokens = _count_total_tokens(train_tok)
    usable_train_tokens = (raw_train_tokens // block_size) * block_size

    train_tok = train_tok.map(lambda ex: _group_texts(ex, block_size), batched=True, batch_size=1000, num_proc=4)
    if eval_tok:
        eval_tok = eval_tok.map(lambda ex: _group_texts(ex, block_size), batched=True, batch_size=1000, num_proc=4)

    collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
    training_args = TrainingArguments(
        output_dir=str(run_dir),
        seed=seed,
        disable_tqdm=True,
        **targs
    )

    pbsz = training_args.per_device_train_batch_size
    gas = training_args.gradient_accumulation_steps
    eff_bsz_runtime = pbsz * gas
    tokens_per_step = eff_bsz_runtime * block_size
    max_steps = training_args.max_steps if training_args.max_steps and training_args.max_steps > 0 else None
    approx_steps_per_epoch = (usable_train_tokens // tokens_per_step) if tokens_per_step > 0 else None

    stats_json = {
        "model_name": model_name,
        "block_size": block_size,
        "per_device_train_batch_size": pbsz,
        "gradient_accumulation_steps": gas,
        "effective_bsz_runtime": eff_bsz_runtime,
        "raw_train_tokens": raw_train_tokens,
        "usable_train_tokens": usable_train_tokens,
        "tokens_per_step": tokens_per_step,
        "approx_steps_per_epoch": approx_steps_per_epoch,
        "max_steps": max_steps,
    }
    with open(run_dir / "train_token_stats.json", "w", encoding="utf-8") as f:
        json.dump(stats_json, f, ensure_ascii=False, indent=2)

    print("[Info] ===== Token Accounting =====")
    print(f"[Info] raw_train_tokens={raw_train_tokens:,}")
    print(f"[Info] usable_train_tokens={usable_train_tokens:,}")
    print(f"[Info] per_step_tokens={tokens_per_step:,}")
    if approx_steps_per_epoch is not None:
        print(f"[Info] approx_steps_per_epoch ≈ {approx_steps_per_epoch:,}")
    if max_steps is not None:
        print(f"[Info] planned_max_steps={max_steps:,}")
    print("=================================")

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_tok,
        eval_dataset=eval_tok,
        data_collator=collator,
        tokenizer=tokenizer,
    )

    if eval_tok is not None:
        csv_path = run_dir / "valid_metrics.csv"
        ppl_cb = ValidPPLLogger(csv_path)
        trainer.add_callback(ppl_cb)
        if "tensorboard" in (targs.get("report_to") or []):
            try:
                from torch.utils.tensorboard import SummaryWriter
                ppl_cb.tb_writer = SummaryWriter(log_dir=str(run_dir))
            except Exception:
                ppl_cb.tb_writer = None

    if tokens_per_step > 0:
        trainer.add_callback(ThroughputLogger(tokens_per_step))

    ckpt_freqs = cfg.get("checkpoint_frequency", None)
    if ckpt_freqs:
        saver_cb = DynamicCheckpointSaver(ckpt_freqs, run_dir)
        trainer.add_callback(saver_cb)

    resume_flag = bool(cfg.get("resume", False))
    resume_ckpt_cfg = cfg.get("resume_checkpoint", None)
    resume_from: Optional[str] = None
    if resume_flag:
        if resume_ckpt_cfg:
            resume_from = _expand(resume_ckpt_cfg)
        else:
            last = _find_last_checkpoint(run_dir)
            if last:
                resume_from = str(last)

    trainer.train(resume_from_checkpoint=resume_from)
    trainer.save_model()
    tokenizer.save_pretrained(run_dir)

    print(f"[OK] Training finished. Checkpoints at: {run_dir}")
    print(f"[OK] Cache stored at: {cache_dir}")
    if eval_tok is not None:
        print(f"[OK] Valid PPL CSV: {run_dir / 'valid_metrics.csv'}")


if __name__ == "__main__":
    main()
