nohup: ignoring input
/home/hd49/miniconda3/envs/gpt2-exp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/home/hd49/miniconda3/envs/gpt2-exp/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
max_steps is given, it will override any value given in num_train_epochs
There were missing keys in the checkpoint model loaded: ['lm_head.weight'].
Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: 
	eval_steps: 50 (from args) != 1000 (from trainer_state.json)
	per_device_train_batch_size: 4 (from args) != 8 (from trainer_state.json)
[Info] Set PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:64
[Info] ===== Training Config =====
[Info] run_id=local_Anone-forward_Pp3-forward
[Info] model=gpt2
[Info] block_size=256, seed=42
[Info] train_files=11 | eval_files=3
[Info] output_dir=/home/hd49/relational-casemarking-learning/checkpoints/local_Anone-forward_Pp3-forward
[Info] cache_dir=/home/hd49/relational-casemarking-learning/cache/local_Anone-forward_Pp3-forward
=================================
[Info] Added 2 special tokens to tokenizer: ['ðŸ„°', 'ðŸ„¿']
[Info] Resized model embeddings to 50259
[Check] token 'ðŸ„°' -> ['ðŸ„°'] | in_vocab=True
[Check] token 'ðŸ„¿' -> ['ðŸ„¿'] | in_vocab=True
[Info] ===== Token Accounting =====
[Info] raw_train_tokens=740,885,088
[Info] usable_train_tokens=740,884,992
[Info] per_step_tokens=16,384
[Info] approx_steps_per_epoch â‰ˆ 45,220
[Info] planned_max_steps=300,000
=================================
/home/hd49/miniconda3/envs/gpt2-exp/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
[Speed] step=75050 ~6,964 tokens/s
{'loss': 4.1389, 'grad_norm': 2.3251967430114746, 'learning_rate': 1.7563598175921347e-05, 'epoch': 3.3579288224980637}
[Speed] step=75100 ~6,992 tokens/s
{'loss': 4.1492, 'grad_norm': 2.641435146331787, 'learning_rate': 1.7560065989814817e-05, 'epoch': 3.3601658776386767}
[Speed] step=75150 ~6,996 tokens/s
{'loss': 4.1418, 'grad_norm': 2.3077850341796875, 'learning_rate': 1.7556531600886554e-05, 'epoch': 3.3624029327792893}
[Speed] step=75200 ~7,013 tokens/s
{'loss': 4.1512, 'grad_norm': 2.4909451007843018, 'learning_rate': 1.7552995010166404e-05, 'epoch': 3.3646399879199023}
[Speed] step=75250 ~7,009 tokens/s
{'loss': 4.1439, 'grad_norm': 2.1488606929779053, 'learning_rate': 1.7549456218684833e-05, 'epoch': 3.3668770430605153}
[Speed] step=75300 ~7,007 tokens/s
{'loss': 4.139, 'grad_norm': 2.327141761779785, 'learning_rate': 1.7545915227472967e-05, 'epoch': 3.369114098201128}
[Speed] step=75350 ~7,011 tokens/s
{'loss': 4.138, 'grad_norm': 2.2682294845581055, 'learning_rate': 1.754237203756256e-05, 'epoch': 3.371351153341741}
[Speed] step=75400 ~6,997 tokens/s
{'loss': 4.1335, 'grad_norm': 2.8946874141693115, 'learning_rate': 1.753882664998602e-05, 'epoch': 3.373588208482354}
[Speed] step=75450 ~7,013 tokens/s
{'loss': 4.1376, 'grad_norm': 2.706725597381592, 'learning_rate': 1.753527906577638e-05, 'epoch': 3.375825263622967}
[Speed] step=75500 ~7,005 tokens/s
{'loss': 4.143, 'grad_norm': 2.279381036758423, 'learning_rate': 1.753172928596733e-05, 'epoch': 3.3780623187635794}
[Speed] step=75550 ~7,011 tokens/s
{'loss': 4.1419, 'grad_norm': 2.629877805709839, 'learning_rate': 1.7528177311593185e-05, 'epoch': 3.3802993739041924}
[Speed] step=75600 ~7,002 tokens/s
{'loss': 4.1455, 'grad_norm': 1.7007893323898315, 'learning_rate': 1.7524623143688905e-05, 'epoch': 3.3825364290448054}
[Speed] step=75650 ~7,007 tokens/s
{'loss': 4.142, 'grad_norm': 2.430204391479492, 'learning_rate': 1.7521066783290087e-05, 'epoch': 3.3847734841854185}
[Speed] step=75700 ~7,011 tokens/s
{'loss': 4.1372, 'grad_norm': 2.2370784282684326, 'learning_rate': 1.7517508231432977e-05, 'epoch': 3.387010539326031}
[Speed] step=75750 ~7,004 tokens/s
{'loss': 4.1579, 'grad_norm': 2.2604215145111084, 'learning_rate': 1.7513947489154443e-05, 'epoch': 3.389247594466644}
[Speed] step=75800 ~7,004 tokens/s
{'loss': 4.1445, 'grad_norm': 2.3181445598602295, 'learning_rate': 1.7510384557492e-05, 'epoch': 3.391484649607257}
[Speed] step=75850 ~7,010 tokens/s
{'loss': 4.1536, 'grad_norm': 2.16629958152771, 'learning_rate': 1.750681943748381e-05, 'epoch': 3.39372170474787}
[Speed] step=75900 ~6,994 tokens/s
{'loss': 4.1476, 'grad_norm': 1.9388835430145264, 'learning_rate': 1.7503252130168657e-05, 'epoch': 3.3959587598884826}
[Speed] step=75950 ~7,012 tokens/s
{'loss': 4.1388, 'grad_norm': 2.235100746154785, 'learning_rate': 1.7499682636585967e-05, 'epoch': 3.3981958150290956}
[Speed] step=76000 ~7,002 tokens/s
{'loss': 4.1455, 'grad_norm': 2.9406955242156982, 'learning_rate': 1.749611095777581e-05, 'epoch': 3.4004328701697086}
{'eval_loss': 4.041355609893799, 'eval_runtime': 909.6359, 'eval_samples_per_second': 135.953, 'eval_steps_per_second': 67.977, 'epoch': 3.4004328701697086}
[Eval] step=76000 eval_loss=4.041356 ppl=56.903
[Speed] step=76050 ~6,985 tokens/s
{'loss': 4.1439, 'grad_norm': 2.6757919788360596, 'learning_rate': 1.749253709477888e-05, 'epoch': 3.4026699253103216}
[Speed] step=76100 ~7,006 tokens/s
{'loss': 4.1396, 'grad_norm': 2.042728900909424, 'learning_rate': 1.7488961048636518e-05, 'epoch': 3.404906980450934}
[Speed] step=76150 ~7,011 tokens/s
{'loss': 4.1456, 'grad_norm': 2.220674991607666, 'learning_rate': 1.7485382820390703e-05, 'epoch': 3.407144035591547}
[Speed] step=76200 ~7,016 tokens/s
{'loss': 4.1512, 'grad_norm': 2.5893537998199463, 'learning_rate': 1.748180241108404e-05, 'epoch': 3.4093810907321602}
[Speed] step=76250 ~7,000 tokens/s
{'loss': 4.1466, 'grad_norm': 1.9777164459228516, 'learning_rate': 1.7478219821759778e-05, 'epoch': 3.4116181458727732}
[Speed] step=76300 ~6,989 tokens/s
{'loss': 4.1313, 'grad_norm': 2.247243642807007, 'learning_rate': 1.7474635053461792e-05, 'epoch': 3.413855201013386}
[Speed] step=76350 ~7,008 tokens/s
{'loss': 4.1441, 'grad_norm': 3.253220558166504, 'learning_rate': 1.74710481072346e-05, 'epoch': 3.416092256153999}
[Speed] step=76400 ~7,000 tokens/s
{'loss': 4.1419, 'grad_norm': 1.9368987083435059, 'learning_rate': 1.746745898412335e-05, 'epoch': 3.418329311294612}
[Speed] step=76450 ~7,004 tokens/s
{'loss': 4.1264, 'grad_norm': 2.8163058757781982, 'learning_rate': 1.7463867685173835e-05, 'epoch': 3.420566366435225}
[Speed] step=76500 ~7,004 tokens/s
{'loss': 4.1336, 'grad_norm': 2.372027635574341, 'learning_rate': 1.7460274211432463e-05, 'epoch': 3.4228034215758374}
[Speed] step=76550 ~7,004 tokens/s
{'loss': 4.1499, 'grad_norm': 2.0551085472106934, 'learning_rate': 1.7456678563946288e-05, 'epoch': 3.4250404767164504}
[Speed] step=76600 ~7,010 tokens/s
{'loss': 4.1401, 'grad_norm': 2.6594784259796143, 'learning_rate': 1.7453080743763e-05, 'epoch': 3.4272775318570634}
[Speed] step=76650 ~7,008 tokens/s
{'loss': 4.1421, 'grad_norm': 2.528289794921875, 'learning_rate': 1.7449480751930915e-05, 'epoch': 3.4295145869976764}
[Speed] step=76700 ~6,993 tokens/s
{'loss': 4.1502, 'grad_norm': 1.9600058794021606, 'learning_rate': 1.744587858949898e-05, 'epoch': 3.431751642138289}
[Speed] step=76750 ~7,001 tokens/s
{'loss': 4.1531, 'grad_norm': 1.8653124570846558, 'learning_rate': 1.7442274257516786e-05, 'epoch': 3.433988697278902}
[Speed] step=76800 ~7,009 tokens/s
{'loss': 4.1446, 'grad_norm': 2.2235474586486816, 'learning_rate': 1.7438667757034547e-05, 'epoch': 3.436225752419515}
[Speed] step=76850 ~7,004 tokens/s
{'loss': 4.1349, 'grad_norm': 2.0664048194885254, 'learning_rate': 1.7435059089103105e-05, 'epoch': 3.438462807560128}
[Speed] step=76900 ~7,009 tokens/s
{'loss': 4.1351, 'grad_norm': 1.9600483179092407, 'learning_rate': 1.7431448254773943e-05, 'epoch': 3.4406998627007406}
[Speed] step=76950 ~7,010 tokens/s
{'loss': 4.1354, 'grad_norm': 2.3974945545196533, 'learning_rate': 1.7427835255099173e-05, 'epoch': 3.4429369178413536}
[Speed] step=77000 ~7,005 tokens/s
{'loss': 4.1327, 'grad_norm': 2.3737270832061768, 'learning_rate': 1.7424220091131536e-05, 'epoch': 3.4451739729819666}
